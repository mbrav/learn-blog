---
layout: post
title: "Reflections on AI"
date: "2017-04-26 16:15:45 -0400"
tags: [AI, biology, evolution]
image: https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Classification_of_images_progress_human.png/640px-Classification_of_images_progress_human.png
description: "The development of (AI) Artificial Intelligence is following the same rules that natural selection has. Will it succeed?"
---


After having all kinds of thoughts about AI, particularly after reading Nick Bostrom's *Superintellegence* a few months ago, I stumbled on Aeon's article [*Raising good robots*](https://aeon.co/essays/creating-robots-capable-of-moral-reasoning-is-like-parenting) by Sally Davis. The article made me reflect on my beliefs on AI.

I think that an AI more intelligent than a human in every single domain — *will* eventually become a reality if the current technologic trends and socio-economic stability continues. Even if you assume any rate of progression, AI is no longer a question of *if*, it becomes a question of *when*. I don't think anyone today can give an answer to the question of *when* an AI breakthrough will occur, but I think it is reasonable to say that it *will* eventually occur, rather than not occur under the current pace of technologic advancement.

Now, this is the base of my belief and which I think is within these schools of beliefs on AI:[^Floridi]

**Singularitarianism** — People who believe in the invention of artificial superintelligence that will abruptly trigger runaway technological growth, resulting in unfathomable changes to human civilization.[^singular]

**AItheism** (*AI-atheism*) — Disbelievers, who don't think that an artificial superintelligencee is possible.

Now, I can consider myself to be closer to the singularitarianist group. While there are plenty of reasons and arguments out there arguing for this view, I have come up with my own which atempts to illustrate why AI lies within the human future.

## Natural Evolution

My reason comes from a rule that allowed life to proliferate on Earth 3 billions ago in the first place. The very fact the sentient, comprehending, and relatively intelligent living beings can read and comprehend these words (assuming your are not robot), who 3 billion years ago where nothing but a part of the Earth's primeval ocean, is something that I think slips past many minds when it comes to thinking about the potential of creating non-biologic intelligence.   

Natural Evolution proves that intelligent life can spawn from a primeval soup of amino acids and molecules. It took more than 3 billion years for single celled organisms to evolve into *Homo Sapiens*, whose brain is the most complicated thing known to science (apart from only black holes and quantum theory perhaps). The human brain is made out of small constituents; cells, molecules, which science can explain on an individual level, yet, it struggles to understand and formulate what happens when all these parts when they start functioning together — the enigma puzzle of the brain.

Life as we know it, is something that has arisen out of relatively simple constituents once it was given some time. I don't see a compelling reason to not believe why something intricate and complex like intelligence will not eventually emerge out of non-biological parts. The marvel of natural selection, is that is able to create sentient life out of biologic material — through the span of billions of trial and error cycles. As long as there is something that can learn from trail and error, take advantage of iterative design, and improve; intelligence does not have to emerge just out of biology.

The same iterative technique that allowed life to emerge, are being actively used computers. All successes in AI are a result of "training" where the AI goes through millions of iterations before its algorithms (also known as neural and convolutional nets) become good at something. For biology, these iterations have taken millions of years; for a computer, it takes a few days. Humans already discovered the *ingredients* to non-biologic intelligence, it is just a matter of time when the *recipe* is found.

Today, an AI algorithm can be trained in a matter of months, or even days, which is unlike the 3 billion years it took for life to evolve into humans. Already such algorithms are better than humans in certain areas such as IBM's speech-recognition AI, which achieved a 5.5% error rate in 2017 — very close to the error rate that humans make when understanding speech which is estimated to be at around a 5.1% error rate.[^ibm] Google in 2016 has also achieved astounding results AI research and has achieved 6.1% error rate when its algorithms classify an image in words.[^google]

![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Classification_of_images_progress_human.png/640px-Classification_of_images_progress_human.png)  
The error rate of machine classification of images such as the one research by Google. Red line — the error rate of a trained human.[^wiki]

Of course, these AI's are referred to as *weak AIs* because they are better than humans at a very narrow domain. A *general AI* is something that is better than humans in many domains, if not all of them. General AI still seems to be nowhere close, but progress does prove that there are fewer reasons to think that it won't eventually become a reality, especially seeing that AI is already proving itself to be better than humans in numerous other domains.

Hope that this was convincing enough to point out that AI wont remain a matter of science fiction forever, and will eventually become a reality. It is only a question of *when*, and not a question of *if*. For a very interesting paper on the subject, check out Nick Bostrom's *How Long Before Superintelligence* paper written in 1997.[^bostrom]

[^singular]: [Technological singularity](https://en.wikipedia.org/wiki/Technological_singularity) from Wikipedia
[^Floridi]: Luciano Floridi in his essay [Should we be afraid of AI?](https://aeon.co/essays/true-ai-is-both-logically-possible-and-utterly-implausible) nicley categorizes the two "schools" of belief about AI proliferation into *Singularitarianism* and *AItheism*.
[^ibm]: [IBM achieves new record in speech recognition](https://www.ibm.com/blogs/research/2017/03/speech-recognition/) from [IBM Research Blog](https://www.ibm.com/us-en/)
[^google]: [Show and Tell: image captioning open sourced in TensorFlow](https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html)
[^wiki]: Source Wikipedia - [Classification of images progress human](https://en.wikipedia.org/wiki/File:Classification_of_images_progress_human.png)
[^bostrom]: Nick Bostrom - [*How Long Before Superintelligence*](http://www.nickbostrom.com/superintelligence.html), 1997. Oxfrord Future of Humanity Institute

*[AI]: short for Artificial Intelligence
